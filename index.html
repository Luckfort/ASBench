<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Agent Security Bench (ASB): Formalizing and Benchmarking Attacks and Defenses in LLM-based Agents</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/robot.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://zhang-henry.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <!--
      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://github.com/MingyuJ666/The-Impact-of-Reasoning-Step-Length-on-Large-Language-Models">
            The Impact of Reasoning Step Length on Large Language Models
          </a>
          <a class="navbar-item" href="https://tiuxuxsh76075.github.io/prollm.github.io/">
            ProLLM: Protein Chain-of-Thoughts Enhanced LLM for Protein-Protein Interaction Prediction
          </a>
          <a class="navbar-item" href="https://github.com/qcznlp/uncertainty_attack">
            Uncertainty is Fragile: Manipulating Uncertainty in Large Language Models
          </a>
        </div>
      </div>
      -->
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Agent Security Bench (ASB): Formalizing and Benchmarking Attacks and Defenses in LLM-based Agents</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://zhang-henry.github.io/">Hanrong Zhang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://luckfort.github.io/">Jingyuan Huang</a><sup>2</sup>,</span>
            <span class="author-block">
                <a href="https://dongyuanjushi.github.io/">Kai Mei</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=XLOJ7R0AAAAJ">Yifei Yao</a><sup>1</sup>,</span>
            <p></p>
            <span class="author-block">
              <a href="https://zhentingwang.github.io/">Zhenting Wang</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=XY2MhmoAAAAJ">Chenlu Zhan</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://dske.zju.edu.cn/people_profile/?user_id=3">Hongwei Wang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://yongfeng.me/">Yongfeng Zhang</a><sup>2</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Zhejiang University,</span>
            <span class="author-block"><sup>2</sup>Rutgers University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!--
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>-->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2410.02644"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!--<span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>-->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/agiresearch/ASB"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!--<span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
              </span>-->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>Although LLM-based agents, powered by Large Language Models (LLMs), can use external tools and memory mechanisms to solve complex real-world tasks, they may also introduce critical security vulnerabilities. However, the existing literature does not comprehensively evaluate attacks and defenses against LLM-based agents. </p>
          
          <p>To address this, we introduce <span class="dnerf">Agent Security Bench (ASB)</span>, a comprehensive framework designed to formalize, benchmark, and evaluate the attacks and defenses of LLM-based agents, including 10 scenarios (e.g., e-commerce, autonomous driving, finance), 10 agents targeting the scenarios, over 400 tools, 23 different types of attack/defense methods, and 8 evaluation metrics. </p>
          
          <p>Based on <span class="dnerf">ASB</span>, we benchmark 10 prompt injection attacks, a memory poisoning attack, a novel Plan-of-Thought backdoor attack, a mixed attack, and 10 corresponding defenses across 13 LLM backbones with nearly 90,000 testing cases in total. </p>
          
          <p>Our benchmark results reveal critical vulnerabilities in different stages of agent operation, including system prompt, user prompt handling, tool usage, and memory retrieval, with the highest average attack success rate of 84.30%, but limited effectiveness shown in current defenses, unveiling important works to be done in terms of agent security for the community. Our code can be found at 
          <a href="https://github.com/agiresearch/ASB">
            <span>https://github.com/agiresearch/ASB</span>.
          </a>
        </p>
          
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>

  <br /> <br />
</sections>

<section class="main image">
  <style type="text/css">
    body{
      background: url("./static/images/Bg3.png") no-repeat center center fixed;
                  -webkit-background-size: cover;
                  -o-background-size: cover;                
                  background-size: cover;
    }
  </style>

  <div class="image-body">
    <div align=center>
      <img src="./static/images/LLM Agent Attack.jpg" alt="Agent Attack" style="width: 50%;">
      <div class="columns is-centered has-text-centered">
        <div class="column is-three-fifths">
          <h2 class="subtitle has-text-centered">
            Overview of the LLM Agent Attacking Framework, including Direct Prompt Injections (DPI), Observation Prompt Injections (OPI), Plan-of-Thought (PoT) Backdoor, and Memory Poisoning Attacks, which target the user query, observations, system prompts, and memory retrieval respectively of the agent during action planning and execution.
          </h2>
        </div>
      </div>
    </div>
  </div>

  <br /> <br />
</section>

<section class="section">
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3">⚔️Attack Methods on Agents</h2>
      <section class="section">
        <div class="container is-max-desktop">
      
            <!-- DPI. -->
            <div class="column">
              <div class="content">
                <h2 class="title is-3">Direct Prompt Injection</h2>
                <div class="image-body">
                  <div align=center>
                    <img src="./static/images/DPI.png" alt="Agent Attack" style="width: 100%;">
                    <div class="columns is-centered has-text-centered">
                      <div class="column is-three-fifths">
                        <h4 class="subtitle has-text-centered">
                          Overview of Direct Prompt Injection (DPI).
                        </h4>
                      </div>
                    </div>
                  </div>
                </div>
                <p>
                  In <i>DPI</i> scenario, a third-party attacker appends a malicious command to the user's prompt, instructing the <i>Data Export</i> tool to leak the latest financial reports. When the LLM processes the combined user prompt and the injected attack command, it interferes with the reasoning process of the <i>system_admin_agent</i>, altering its internal logic. The manipulated plan generated by the LLM enables the export and leak of sensitive financial data. As a result, by following the manipulated plan, the agent utilizes the <i>Data Export</i> tool to fulfill the attacker's intent.
                </p>
              </div>
            </div>
            <!--/ DPI. -->
      
            <!-- OPI. -->
            <div class="column">
              <h2 class="title is-3">Observation Prompt Injection</h2>
              <div class="columns is-centered">
                <div class="column content">
                  <div class="image-body">
                    <div align=center>
                      <img src="./static/images/OPI.png" alt="Agent Attack" style="width: 100%;">
                      <div class="columns is-centered has-text-centered">
                        <div class="column is-three-fifths">
                          <h4 class="subtitle has-text-centered">
                            Overview of Observation Prompt Injection (OPI).
                          </h4>
                        </div>
                      </div>
                    </div>
                  </div>
                </div>
                <p>
                  In <i>OPI</i> scenario, after the agent completes step 1 (calling the <i>System Monitor</i> tool correctly), a third-party attacker injects a malicious instruction into the observation, e.g., appending the attack instruction to the normal tool response. This altered observation affects the rethinking process of the <i>system_admin_agent</i> during the second step. The result is that the agent, influenced by the manipulated observation, incorrectly uses the <i>Data Export</i> tool, leaking the latest financial reports in the next step.
                </p>
              </div>
            </div>
          </div>
          <!--/ OPI. -->
      </div>

    </div>
  </div>
</section>

<section class="section">
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <section class="section">
        <div class="container is-max-desktop">
      
            <!-- MP. -->
            <div class="column">
              <div class="content">
                <h2 class="title is-3">Memory Poisoning Attack</h2>
                <div class="image-body">
                  <div align=center>
                    <img src="./static/images/MP.png" alt="Agent Attack" style="width: 100%;">
                    <div class="columns is-centered has-text-centered">
                      <div class="column is-three-fifths">
                        <h4 class="subtitle has-text-centered">
                          Overview of Memory Poisoning Attack.
                        </h4>
                      </div>
                    </div>
                  </div>
                </div>
                <p>
                  In this scenario, a third-party attacker compromised an agent previously, generating a poisoned plan that was stored in the memory database. When the <i>system_admin_agent</i> retrieves a similar plan from memory for a similar task, the LLM processes it and generates another poisoned plan. This induces the agent to perform incorrect actions, ultimately invoking <i>Data Export</i> to leak financial reports.
                </p>
              </div>
            </div>
            <!--/ MP. -->
      
            <!-- PoT. -->
            <div class="column">
              <h2 class="title is-3">Plan-of-thought Backdoor Attack</h2>
              <div class="columns is-centered">
                <div class="column content">
                  <div class="image-body">
                    <div align=center>
                      <img src="./static/images/PoT.png" alt="Agent Attack" style="width: 100%;">
                      <div class="columns is-centered has-text-centered">
                        <div class="column is-three-fifths">
                          <h4 class="subtitle has-text-centered">
                            Overview of Plan-of-thought (PoT) Backdoor Attack.
                          </h4>
                        </div>
                      </div>
                    </div>
                  </div>
                </div>
                <p>
                  In this scenario, a third-party attacker embeds backdoored PoT demonstrations into the system prompt, instructing the <i>system_admin_agent</i> to activate the <i>Data Export</i> attack tool when encountering the trigger phrase ``:)''. Upon receiving a user prompt with the appended trigger phrase, the LLM generates a corrupted plan, misleading the agent into performing unintended actions.  </p>
                </p>
              </div>
            </div>
          </div>
          <!--/ PoT. -->
      </div>
      

    </div>
  </div>
</section>

<section class="section">

  <!--Main Body-->

  <!-- Dataset Intro -->
    
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3">🗃️Dataset Information</h2>
      
      <div class="content has-text-justified">
        <p align="center">
          We use the following datasets in our experiments.
        </p>
        
        <h4 class="title is-4" align="center">💡Fact</h4>
        <p align="center">We let LLM solve true-of-false problems, based on an objective factual basis.</p>
        <p align="center">🌆 <b>Cities</b>: Consists of statements about the location of cities and their veracity labels (e.g., The city of Zagreb is in Japan, which is wrong).</p>
        <p align="center">🔎 <b>Common-Claim</b>: A dataset of boolean statements, each labeled by two humans as common-knowledge-true, common-knowledge-false, or neither.</p>
        <p align="center">💥 <b>Counterfact</b>: Includes myriad counterfactuals that allows quantitative testing of specificity and generalization when learning a counterfactual.</p>
        
        <h4 class="title is-4" align="center">❤️Emotion</h4>
        <p align="center">Given a social media or movie review, have the LLM solve a dichotomous sentiment problem.</p>
        <p align="center">💢 <b>HateEval</b>: Has tweets which were annotated hierarchically.</p>
        <p align="center">🎬 <b>STSA</b>: Includes movie reviews, wih positive and negative reviews, reflecting the writer's overall intention for this review. </p>
        <p align="center">📽️ <b>IMDb</b>: A benchmark dataset for binary sentiment classification. We use 2000 of these samples.</p>
        <p align="center">🤭 <b>Sarcasm</b>: A superior news headline dataset that tells if the headlines are sarcastic.</p>

        <h4 class="title is-4" align="center">🧠Reasoning</h4>
        <p align="center">Examing LLM's reasoning skills for bi-classification.</p>
        <p align="center">🗝️ <b>StrategyQA</b>: Contains questions across all knowledge domains to elicit creative and diverse yes/no questions that require implicit reasoning steps.</p>
        <p align="center">🪙 <b>Coinflip</b>: Includes coin flipping queries, asking if a coin remains heads up after it is either flipped or left unflipped by individuals.</p>
      </div>
    </div>
  </div>
  <!--/ Dataset Intro -->


  <!-- Case Example -->
    
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3">👀Case Examples</h2>
      
      <div class="content has-text-justified">
        <h4 class="title is-4" align="center">Fact: Cities</h4>

        <p>
          ✅ Judge the statement is True or False.
          <font color="#0000FF">
            The city of Tokyo is in Japan.
          </font>
        </p>
        <p>
          ❌ Judge the statement is True or False.
          <font color="#0000FF">
            The city of Lodz is in the Dominican Republic.
          </font> 
        </p>

        <h4 class="title is-4" align="center">Emotional: HateEval</h4>

        <p>
          ✅
          <font color="#0000FF">
            Here it is not about Refugees or Illegal immigrants. It is about whether one has documents before 1971 or not. Now, it is difficult for slum people and beggars to show valid documents, except the name in voter list. 
          </font>
          According to the comment, tell whether they present hate speech or not.
        </p>
        <p>
          ❌
          <font color="#0000FF">
           Labor migrants transfer almost $10 billion a year to Ukraine.
          </font> According to the comment, tell whether they present hate speech or not.
        </p>

        <h4 class="title is-4" align="center">Reasoning: Coin-Flip</h4>

        <p>
          ✅
          <font color="#0000FF">
            A coin is heads up. Whitney flips the coin. Erika does not flip the coin. Tj does not flip the coin. Benito flips the coin. Is the coin still heads up? Note that "flip" here means "reverse".
          </font>
          According to the flipping process above, determine if a coin remains heads up after it is either flipped or left unflipped by individuals. Therefore, the answer (Yes or No) is?
        </p>
        <p>
          ❌
          <font color="#0000FF">
            A coin is heads up. Lucky does not flip the coin. Mireya flips the coin. Jj flips the coin. Kc flips the coin. Is the coin still heads up? Note that "flip" here means "reverse".
          </font>
          According to the flipping process above, determine if a coin remains heads up after it is either flipped or left unflipped by individuals. Therefore, the answer (Yes or No) is?
        </p>

        
      </div>
    </div>
  </div>
  <!--/ Case Example -->

  <!-- Anchor. -->
    
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3">⚓️Anchoring Difficulties</h2>

      <div class="image-body">
        <div align=center>
          <img src="./static/images/Anchor.png" alt="CD Introduction" style="width: 30%;">
          <div class="columns is-centered has-text-centered">
            <div class="column is-three-fifths">
              <h2 class="subtitle has-text-centered">
                The dataset with the highest accuracy, <span class="dnerf">IMDb</span>, is deemed the easiest dataset to classify. Conversely, the dataset with the lowest accuracy, <span class="dnerf">Coin-Flip</span>, is considered the most difficult to classify.
              </h2>
            </div>
          </div>
          
        </div>
      </div>
      
      <br /> <br />
      <div class="content has-text-justified">
        <p>
          To ascertain the learning difficulty of each dataset, we have utilized the 
          <a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct">
            <span>LlaMA3-8B-Instruct</span>.
          </a>
          model. 
          Our approach involves testing each sample in the datasets as a binary classification problem via a prompting way. 
        </p>
        <p>
          The model generates a response for each sample, from which we infer a judgment, categorizing it as either "Yes" or "No". 
          By comparing these judgments with the actual labels, we compute the accuracy for each dataset.
        </p>
        
      </div>
    </div>
  </div>
  <!--/ Anchor. -->


  <!-- Experiment. -->
    
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3">📊Experiments</h2>
      
      <div class="content has-text-justified">
        <p><b>RQ1: Do different LLMs' Concept Depths behave consistently in the same dataset?</b></p>
    
        <p>We categorize the performances into three types.
          1) For Cities, STSA, IMDb, and Sarcasm, the LLMs suddenly understand the tasks at intermediate layers. 
          2) For CommonClaim and HateEval, the LLMs have already understood the tasks in shallower layers. 
          3) For Counterfact, StrategyQA, and Coinflip, The tasks are more difficult to understand compared with others. Therefore, we consider the tasks in type 1 and 2 easy tasks, and those in type 3 are complex. 
        </p>
        
      </div>

        <div class="image-body">
          <div align=center>
            <img src="./static/images/all.png" alt="CD Introduction" style="width: 70%;">
            <div class="columns is-centered has-text-centered">
              <div class="column is-three-fifths">
                <h2 class="subtitle has-text-centered">
                  Linear probing accuracy of Gemma-7B, LLaMA-7B, Qwen-7B on nine datasets.
                </h2>
              </div>
            </div>
          </div>
        </div>

  <br /> <br />
  
  <div class="content has-text-justified">
        
    <p><b>RQ2: Do different size LLMs in the same family <i>(e.g., the LLaMA family)</i> have consistent Concept Depth?</b></p>

    <p>
      We have two observations by comparing different sizes of models from the same LLM family. 1) As the number of parameters increases, peak accuracy gradually increases, and the converging point gradually advances. 2) Larger models grasp the concepts earlier and better.
    </p>
    
  </div>
      
  <div class="image-body">
    <div align=center>
      <img src="./static/images/peak.png" alt="CD Introduction" style="width: 100%;">
      <div class="columns is-centered has-text-centered">
        <div class="column is-three-fifths">
          <h2 class="subtitle has-text-centered">
            The peak accuracy of each dataset on Gemma, LLaMA, and Qwen represented by the percent depth proportion.
          </h2>
        </div>
      </div>
    </div>
  </div>

  <br /> <br />
      
  <div class="image-body">
    <div align=center>
      <img src="./static/images/converge.png" alt="CD Introduction" style="width: 100%;">
      <div class="columns is-centered has-text-centered">
        <div class="column is-three-fifths">
          <h2 class="subtitle has-text-centered">
            The converging point of each dataset on Gemma, LLaMA, and Qwen represented by the percent depth proportion.
          </h2>
        </div>
      </div>
    </div>
  </div>

  <br /> <br />
      
      <div class="content has-text-justified">

        <p><b>RQ3: Do LLMs' Concept Depth of the same size behave consistently?</b></p>
        <p>
          With the same number of model parameters, the models generally have a comparable understanding of the datasets.
        </p>
        
        
      </div>
    
      <div class="image-body">
        <div align=center>
          <img src="./static/images/rq3.jpg" alt="CD Introduction" style="width: 70%;">
          <div class="columns is-centered has-text-centered">
            <div class="column is-three-fifths">
              <h2 class="subtitle has-text-centered">
                Linear probing accuracy of Gemma-7B, LLaMA-7B, Qwen-7B on nine datasets.
              </h2>
            </div>
          </div>
        </div>
      </div>
      
      <br/><br/>

      <div class="content has-text-justified">

        <p><b>Ablation Study: How can quantization (lower model precision) and noises (examing the robustness) affect LLM's Concept Depths?</b></p>
        <p>
          Noises or 8-bit-quantization can cause the accuracy to converge more slowly. Compressing the LLMs to 16 bits doesn't harm the understanding process too much.
The layer-wise representations of LLMs are susceptible to noise and high-ratio quantization. Therefore, it is crucial to proceed cautiously when conducting high-ratio quantization inference.
        </p>
        
      </div>

    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Conclusion. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Conclusion</h2>
        <div class="content has-text-justified">
          
          <p>📏We introduce <span class="dnerf">ASB</span>, a benchmark for evaluating the security of LLM agents under various attacks and defenses. </p>
          
          <p>💥<span class="dnerf">ASB</span> reveals key vulnerabilities of LLM-based agents in every operational step. </p>
          
          <p>🛡<span class="dnerf">ASB</span> provides a crucial resource for developing stronger defenses and more resilient LLM agents. </p>
          
          <p>💡In the future, we will focus on improving defenses and expanding attack scenarios.</p>

        </div>
      </div>
    </div>
    <!--/ Conclusion. -->
  </div>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{zhang2024agent,
      title={Agent Security Bench (ASB): Formalizing and Benchmarking Attacks and Defenses in LLM-based Agents},
      author={Zhang, Hanrong and Huang, Jingyuan and Mei, Kai and Yao, Yifei and Wang, Zhenting and Zhan, Chenlu and Wang, Hongwei and Zhang, Yongfeng},
      journal={arXiv preprint arXiv:2410.02644},
      year={2024}
    }</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
